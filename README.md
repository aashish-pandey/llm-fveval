# LLM-FVEval

This repository implements an **FVEval-style framework** for evaluating the
formal quality of SystemVerilog assertions using formal verification tools.

The goal is **evaluation, not generation**.

In the current baseline experiment, assertions are generated by a single
Large Language Model (LLM) and then evaluated using **Cadence JasperGold**
to understand their formal usefulness.

---

## What Are We Doing?

At a high level, this project answers a simple question:

> **How good are assertions generated by an LLM when judged using formal verification?**

To study this, we follow a minimal and controlled workflow.

---

## Baseline Experiment (Phase-0)

### Step 1: Assertion Generation
- A **single LLM** is used to generate SystemVerilog Assertions (SVAs)
- The input design is a **simple synchronous FIFO**
- The prompt is intentionally minimal
- No human post-processing or cleanup is performed

The generated assertions are treated as **raw experimental artifacts**
and are frozen before evaluation.

---

### Step 2: Formal Evaluation (FVEval-style)
- Assertions are evaluated using **Formal Property Verification (FPV)**
- Cadence **JasperGold** is used as the formal engine
- Each assertion is classified based on formal outcomes, such as:
  - syntactic validity
  - proven properties
  - failing properties
  - vacuous properties
  - undetermined / timeout cases

This evaluation is inspired by **industry FVEval-style methodologies**,
where assertion quality is judged by formal behavior rather than
textual similarity or human inspection.

---

## Why FIFO?

FIFO is used as the initial benchmark because:
- It has well-known safety and consistency properties
- It exposes common pitfalls such as environment assumptions
- It is simple enough to allow precise formal analysis
- It is widely used and easily understood

Additional designs can be added incrementally in the same framework.

---

## What This Repo Is (and Is Not)

### This repo **IS**:
- An implementation of an **FVEval-style evaluation workflow**
- A reproducible experimental setup for formal assertion evaluation
- A baseline for future comparison (multi-LLM, ensemble methods, etc.)

### This repo **IS NOT**:
- A claim that LLMs generate perfect assertions
- A new formal verification tool
- An assertion synthesis framework
- A benchmark claiming superiority over humans or industry methods

---


Each benchmark is self-contained and treated as a controlled experiment.

---

## Current Status

- ✔ FIFO benchmark defined
- ✔ Single-LLM assertion generation completed
- ✔ Assertions frozen as experimental input
- ⏳ Formal evaluation and metric extraction in progress

---

## Future Directions

This framework is designed to support:
- multiple LLMs
- ensemble-based assertion selection
- comparison against existing ensemble systems
- extended FVEval-style metrics
- additional RTL benchmarks

---

## Tools

- **LLM inference**: Ollama (local)
- **Formal verification**: Cadence JasperGold
- **Language**: SystemVerilog (SVA)

---

## License

This project is intended for academic and research use.

